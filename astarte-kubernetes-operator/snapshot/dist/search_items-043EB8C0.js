searchNodes=[{"doc":"Astarte Kubernetes Operator runs and manages an Astarte Cluster in a Kubernetes Cluster. It is meant to work on any Managed Kubernetes installation, and leverages a number of Kubernetes features to ensure Astarte runs as smooth as possible. It also handles upgrades, monitoring, and more. This guide is meant for System Administrators who need to deal with Astarte clusters installation, maintenance and upgrade. The guide will cover prerequisites, installation in different supported environments, and common maintenance operations. Astarte Operator is the foundation of any Astarte installation, and you can find more information about it in the following sections. Before you begin, it is fundamental you are familiar with Astarte's architecture, design and concepts , especially for what concerns its components and 3rd party services. Compatibility Matrix Astarte Operator Version Astarte Version Kubernetes Version v1.0.0 v0.11 - v1.0 v1.19+ v1.0.x v0.11 - v1.0 v1.19+ v22.11 v1.0+ v1.22+ v23.5 v1.0+ v1.22+ v24.5 v1.0+ v1.24+ v25.5 v1.0+ v1.24+ Notes : starting from Kubernetes 1.22, the AstarteVoyagerIngress resource is not supported anymore; starting from Astarte Operator v24.5.0+, the support for AstarteVoyagerIngress is removed.","ref":"001-intro_administrator.html","title":"Introduction","type":"extras"},{"doc":"Astarte is a Native Kubernetes application, and as such Kubernetes is a hard requirement. It is possible to run Astarte outside Kubernetes, although a number of features won't be available - these setups are outside the scope of this document.","ref":"010-system_requirements.html","title":"System Requirements","type":"extras"},{"doc":"Astarte requires at least Kubernetes 1.19, and strives to be compatible with all newer Kubernetes versions. It is advised to consult Astarte Operator's compatibility matrix in the README to ensure a specific Kubernetes setup is supported. The Astarte Operator does not require any unstable feature gate in Kubernetes 1.19, and is actively tested against KinD and major Managed Kubernetes installations on various Cloud Providers.","ref":"010-system_requirements.html#kubernetes-requirements","title":"System Requirements - Kubernetes Requirements","type":"extras"},{"doc":"Astarte Operator requires cert-manager to enable Webhooks. This documentation will detail all needed steps for installing cert-manager in the cluster in case it's not installed yet.","ref":"010-system_requirements.html#dependencies","title":"System Requirements - Dependencies","type":"extras"},{"doc":"Depending on the kind of setup, Astarte might require different resource configurations when it comes to nodes. In addition, if one is planning on a redundant setup, a minimum of 3 physical nodes is required. To spin up a testing instance you should allocate at least 4 vCPU and 8GB of memory just for Astarte. Please keep in mind that additional applications running within the cluster and the Kubernetes overhead itself must be accounted when sizing your Astarte cluster.","ref":"010-system_requirements.html#resource-requirements","title":"System Requirements - Resource Requirements","type":"extras"},{"doc":"As much as Astarte's Operator is capable of creating a completely self-contained installation, there's a number of prerequisites to be fulfilled depending on the use case.","ref":"020-prerequisites.html","title":"Prerequisites","type":"extras"},{"doc":"The following tools are required within your local machine: kubectl : must be compatible with your target Kubernetes version, astartectl : your version must be the same of the Astarte Operator running in your cluster, helm : v3 is required.","ref":"020-prerequisites.html#on-your-machine","title":"Prerequisites - On your machine","type":"extras"},{"doc":"Astarte currently features only one supported Managed Ingress, based on NGINX . NGINX provides routing, SSL termination and more, and as of today is the preferred/advised way to run Astarte in production. Astarte Operator is capable of interacting with NGINX through its dedicated AstarteDefaultIngress resource, as long as an NGINX ingress controller is installed. Installing the ingress controller is as simple as running a few helm commands: $ helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx $ helm repo update $ helm install ingress-nginx ingress-nginx/ingress-nginx -n ingress-nginx \\ --set controller.service.externalTrafficPolicy=Local \\ --create-namespace Please, be aware that trying to deploy multiple ingress controllers in your cluster may result in all of them trying simultaneously to handle the Astarte ingress resource. Consider using ingress classes for avoiding confusing situations as outlined here . In the end, you won't need to create NGINX ingresses yourself: the Astarte Operator itself will take care of this task.","ref":"020-prerequisites.html#nginx","title":"Prerequisites - NGINX","type":"extras"},{"doc":"For production environments, consider using RabbitMQ deployed by the [RabbitMQ Cluster Operator] ( https://www.rabbitmq.com/kubernetes/operator/operator-overview ) or any other managed solution that you prefer. The Astarte Operator includes only basic management of RabbitMQ, which is deprecated since v24.5 and as such it should not be relied upon when dealing with production environments. Futher details can be found here .","ref":"020-prerequisites.html#rabbitmq","title":"Prerequisites - RabbitMQ","type":"extras"},{"doc":"Astarte requires cert-manager to be installed in the cluster in its default configuration (installed in namespace cert-manager as cert-manager ). If you are using cert-manager in your cluster already you don't need to take any action - otherwise, you will need to install it. Astarte is actively tested with cert-manager 1.16.3, but should work with any 1.0+ releases of cert-manager . If your cert-manager release is outdated, please consider upgrading to a newer version according to this guide . cert-manager documentation details all needed steps to have a working instance on your cluster. However, in case you won't be using cert-manager for other components beyond Astarte or, in general, if you don't have very specific requirements, it is advised to install it through its Helm chart. To do so, run the following commands: $ helm repo add jetstack https://charts.jetstack.io $ helm repo update $ kubectl create namespace cert-manager $ helm install \\ cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --version v1.16.3 \\ --set crds.enabled=true This will install cert-manager and its CRDs in the cluster.","ref":"020-prerequisites.html#cert-manager","title":"Prerequisites - cert-manager","type":"extras"},{"doc":"In production deployments, it is strongly advised to have a separate Cassandra cluster interacting with the Kubernetes installation. This is due to the fact that Cassandra Administration is a critical topic, especially with mission critical workloads. Astarte Operator includes only basic management of Cassandra, which is deprecated since v1.0 and as such it should not be relied upon when dealing with production environments. Furthermore, in the near future, Cassandra support is planned to be removed from Astarte Operator in favor of the adoption of a dedicated Kubernetes Operator (e.g. Scylla Operator ). In case an external Cassandra cluster is deployed, be aware that Astarte lives on the assumption it will be the only application managing the Cluster - as such, it is strongly advised to have a dedicated cluster for Astarte.","ref":"020-prerequisites.html#external-cassandra-scylla","title":"Prerequisites - External Cassandra / Scylla","type":"extras"},{"doc":"When deploying external components, it is important to take in consideration how Kubernetes behaves with the underlying infrastructure. Most modern Cloud Providers have a concept of Virtual Private Cloud, by which the internal Kubernetes Network stack directly integrates with their Network stack. This, in short, enables deploying Pods in a shared private network, in which other components (such as Virtual Machines) can be deployed. This is the preferred, advised and supported configuration. In this scenario, there's literally no difference between interacting with a VM or a Pod, enabling a hybrid infrastructure without having to pay the performance cost.","ref":"020-prerequisites.html#kubernetes-and-external-components","title":"Prerequisites - Kubernetes and external components","type":"extras"},{"doc":"The most simple and common installation procedure exploits the Astarte Operator's Helm chart . Helm is intended to be used as the operator's lifecycle management tool, thus make sure you are ready with a working Helm installation . Please, before starting with the Operator's install procedure make sure that any prerequisite has been satisfied.","ref":"030-installation_kubernetes.html","title":"Installing Astarte Operator","type":"extras"},{"doc":"Installing the Operator is as simple as $ helm repo add astarte https://helm.astarte-platform.org $ helm repo update $ helm install astarte-operator astarte/astarte-operator -n astarte-operator This command will take care of installing all needed components for the Operator to run. This includes all the RBAC roles, Custom Resource Definitions, Webhooks, and the Operator itself. You can use the --version switch to specify a version to install. When not specified, the latest stable version will be installed instead.","ref":"030-installation_kubernetes.html#installation","title":"Installing Astarte Operator - Installation","type":"extras"},{"doc":"The procedure for upgrading the Operator depends on the version of the Operator you want to upgrade from. Please refer to the Upgrade Guide section that fits your needs.","ref":"030-installation_kubernetes.html#upgrading-the-operator","title":"Installing Astarte Operator - Upgrading the Operator","type":"extras"},{"doc":"Uninstalling the Operator is as simple as: $ helm uninstall astarte-operator -n astarte-operator Starting from v24.5.0, the removal of the Operator preserves the Astarte, AstarteDefaultIngress and Flow CRDs. To prevent unwanted deletion of the deployed custom resources, the removal of the CRDs must be performed manually. Please be aware that the Operator is meant to handle the full lifecycle of the Astarte, AstarteDefaultIngress and Flow resources. If your services are still up and running when the Operator is uninstalled you might experience limited functionalities (e.g. even if flow creation succeeds, there is no guarantee that the flow will actually start).","ref":"030-installation_kubernetes.html#uninstalling-the-operator","title":"Installing Astarte Operator - Uninstalling the Operator","type":"extras"},{"doc":"In case you do not want to use helm to manage the Operator, this guide will run you through all the steps needed to set up Astarte Kubernetes. To come along with this guide, the following components are required: operator-sdk kustomize Please make sure that the version of operator-sdk matches the version used by the Astarte Operator (the detail can be found within the project's CHANGELOG ). Moreover, please make sure that the cluster kubectl is pointing to is the one you want to target with the installation. Note: Please be aware that this method is to be used only if you have very specific reasons why not to use helm , for example: you're running a fork of the Operator, you're running the Operator outside of the cluster, or you're on the very bleeding edge. helm automates internally all of this guide and should be your main choice in production.","ref":"040-manual_kubernetes.html","title":"Manual Operator Installation","type":"extras"},{"doc":"First of all, you will need to clone the Operator repository, as this is where some of the needed resources for the Operator are. Ensure you're cloning the right branch for the Operator Version you'd like to install, in this case release-1.0 : git clone https://github.com/astarte-platform/astarte-kubernetes-operator.git cd astarte-kubernetes-operator","ref":"040-manual_kubernetes.html#clone-the-operator-repository","title":"Manual Operator Installation - Clone the Operator Repository","type":"extras"},{"doc":"The Operator requires a number of RBAC roles to run, and will also require Astarte CRDs to be installed. To install all the required components, simply run: make install","ref":"040-manual_kubernetes.html#install-rbacs-and-crds","title":"Manual Operator Installation - Install RBACs and CRDs","type":"extras"},{"doc":"Running the Operator inside the cluster is as simple as executing the following: make deploy Actually, the above command does more than just deploying the Operator, as it also install RBACs, CRDs. The deployment therefore can be performed in just one command. To check if the deployment is successful: kubectl get deployment -n astarte-operator astarte-operator","ref":"040-manual_kubernetes.html#running-the-operator-inside-the-cluster","title":"Manual Operator Installation - Running the Operator inside the Cluster","type":"extras"},{"doc":"Note: Running the operator outside the cluster is not advised in production. Usually, you need such a deployment if you plan on developing the Operator itself. However, this scenario is tested in the e2e tests, and as such provides the very same features of the in-cluster Deployment, which remains the go-to scenario for production. From the root directory of your clone, run: make run ENABLE_WEBHOOKS=false This will bring up the Operator (with all the webhooks disabled) and connect it to your current Kubernetes context. Please, refer to the OperatorSDK documentation to have further insights on this scenario.","ref":"040-manual_kubernetes.html#running-the-operator-outside-the-cluster","title":"Manual Operator Installation - Running the Operator outside the Cluster","type":"extras"},{"doc":"Astarte heavily requires SSL in a number of interactions, even though this can be bypassed with ssl: false . In general, there are two alternative scenarios when dealing with certificates: you already purchased SSL certificates for your domains, you want your certificates to be handled by Let's Encrypt through cert-manager. The two alternative procedures for securing your Astarte deployment are outlined in the following sections.","ref":"050-handling_certificates.html","title":"Handling Astarte certificates","type":"extras"},{"doc":"If you already own certificates for your domains, all it's needed is creating a TLS secret in the namespace in which Astarte resides. Assuming that the certificate and key are saved respectively as cert.pem and privkey.pem , simply run: $ kubectl create secret tls astarte-tls-cert -n astarte \\ --cert=cert.pem --key=privkey.pem","ref":"050-handling_certificates.html#use-your-own-certificates","title":"Handling Astarte certificates - Use your own certificates","type":"extras"},{"doc":"The process of obtaining a TLS certificate from Let's Encrypt is handled by cert-manager using a cluster issuer. The issuer will query the Let's Encrypt API and handles the challenge to confirm that you are the right owner of the specified domain. Two types of challenges are supported, namely DNS01 and HTTP01. Ensure all the prerequisites are satisfied and that both cert-manager and the NGINX ingress controller are deployed within your cluster. If you haven't installed them yet, you can do it with these simple commands: install cert-manager: $ helm repo add jetstack https://charts.jetstack.io $ helm repo update $ kubectl create namespace cert-manager $ helm install \\ cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --version v1.7.0 \\ --set installCRDs=true install NGINX ingress controller: $ helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx $ helm repo update $ helm install ingress-nginx ingress-nginx/ingress-nginx -n ingress-nginx \\ --set controller.service.externalTrafficPolicy=Local \\ --create-namespace HTTP01 Challenge The current section outlines the procedure for setting up a ClusterIssuer to solve the HTTP01 challenge. Find the external IP assigned to the ingress controller Knowing the external IP of the NGINX ingress controller is crucial for solving the HTTP01 challenge. You can find the external IP under the EXTERNAL-IP field when inspecting the output of the following command: $ kubectl get svc -n ingress-nginx ingress-nginx-controller Configure your DNS Once the external IP of the ingress controller is known, make sure all your Astarte domains point to the NGINX Ingress controller IP. In particular, the list of the domains is: api.your-domain.example.com dashboard.your-domain.example.com (if deployed) broker.your-domain.example.com Create a ClusterIssuer Define a ClusterIssuer and save it as cluster-issuer.yaml : apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: your-email@email.com privateKeySecretRef: name: letsencrypt solvers: - http01: ingress: class: nginx Then, apply the resource with the following: $ kubectl apply -f cluster-issuer.yaml Create a Certificate resource Once the ClusterIssuer has been created, add a Certificate resource in the Astarte namespace referencing the ClusterIssuer , and save it as certificate.yaml : apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: astarte-default-ingress-certificate namespace: astarte spec: dnsNames: - &lt;your-dns.names&gt; secretName: astarte-tls-cert issuerRef: name: letsencrypt kind: ClusterIssuer Then, apply the Certificate resource: $ kubectl apply -f certificate.yaml Wait the HTTP challenge to complete As soon as the HTTP challenge completes, a Kubernetes secret of type kubernetes.io/tls called astarte-tls-cert will be created in the astarte namespace. Now you can reference the TLS secret in both the Astarte and AstarteDefaultIngress resources where required. DNS01 challenge The current section describes the procedure for setting up a ClusterIssuer to use Google CloudDNS to solve the DNS01 challenge. Therefore, when needed, the rest of this section will make use of the gcloud CLI . If your Astarte deployment is hosted by another cloud provider, please refer to the cert-manager specific documentation . Define a DNS Zone for your project First, ensure that a DNS Zone is already defined for your project . If this requirement is not satisfied, this page provides guidance for the creation of the DNS Zone for a project hosted on Google Cloud. If your cluster is hosted by any other cloud provider, please ensure to follow the needed steps to fulfill the requirement. Set up a Service Account with privileges of DNS Administrator To set up a service account with privileges of DNS Administrator, run the following command: $ PROJECT_ID=&lt;your-project-id&gt; $ gcloud iam service-accounts create dns01-solver --display-name &quot;dns01-solver&quot; $ gcloud projects add-iam-policy-binding $PROJECT_ID \\ --member serviceAccount:dns01-solver@$PROJECT_ID.iam.gserviceaccount.com \\ --role roles/dns.admin Create a Service Account secret To access the service account, cert-manager uses a key stored in a Kubernetes Secret. Therefore, create a key and download it as a json file: $ gcloud iam service-accounts keys create key.json \\ --iam-account dns01-solver@$PROJECT_ID.iam.gserviceaccount.com and create a secret named clouddns-dns01-solver-svc-acct in the cert-manager namespace from the key.json file: $ kubectl create secret generic -n cert-manager \\ clouddns-dns01-solver-svc-acct \\ --from-file=key.json Create a ClusterIssuer that uses CloudDNS Define a ClusterIssuer resource which uses the secret, and save it as cluster-issuer.yaml : apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-dns-cluster-issuer spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: your-email@email.com privateKeySecretRef: # Secret resource that will be used to store the account's private key. name: letsencrypt-cluster-issuer-key solvers: - dns01: cloudDNS: # The ID of the GCP project project: &lt;your-project-id&gt; # This is the secret used to access the service account serviceAccountSecretRef: name: clouddns-dns01-solver-svc-acct key: key.json Apply the resource simply running the following: $ kubectl apply -f cluster-issuer.yaml Create a Certificate resource Once the ClusterIssuer has been created, add a Certificate resource in the Astarte namespace referencing the ClusterIssuer , and save it as certificate.yaml : apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: astarte-default-ingress-certificate namespace: astarte spec: dnsNames: - &lt;your-dns.names&gt; secretName: astarte-tls-cert issuerRef: name: letsencrypt-dns-cluster-issuer kind: ClusterIssuer Thus, apply the Certificate resource: $ kubectl apply -f certificate.yaml Wait the DNS challenge to complete As soon as the DNS challenge completes, a Kubernetes secret of type kubernetes.io/tls called astarte-tls-cert will be created in the astarte namespace. Now you can reference the TLS secret in both the Astarte and AstarteDefaultIngress resources where required.","ref":"050-handling_certificates.html#use-let-s-encrypt-certificates-with-cert-manager","title":"Handling Astarte certificates - Use Let's Encrypt certificates with cert-manager","type":"extras"},{"doc":"The current page describes how to handle SSL certificates for securing your Astarte instance. In particular the following use cases are analyzed: certificates have already been purchased and needs to be properly deployed, let cert-manager generate and handle certificates in the following cases: solving HTTP01 ACME challenges, solving DNS01 ACME challenges. At the end of each procedure you will end up with a Kubernetes TLS secret, named astarte-tls-cert , deployed in the Astarte namespace. Reference the secret in your Astarte and AstarteDefaultIngress resources where required to secure your Astarte deployment.","ref":"050-handling_certificates.html#conclusions","title":"Handling Astarte certificates - Conclusions","type":"extras"},{"doc":"Once the Astarte Operator has been installed , and any prerequisite has been fulfilled , you can move forward and deploy an Astarte Cluster.","ref":"060-setup_cluster.html","title":"Setting up the Cluster","type":"extras"},{"doc":"The standard way of deploying an Astarte instance is by creating your own Astarte Custom Resource. This gives you an high degree of customization, allowing you to tweak any single parameter in the Astarte setup. The main Astarte CRD is extensively documented and the available fields can be inspected here . To create your Astarte resource, just create your Astarte Custom Resource, which will look something like this: apiVersion: api.astarte-platform.org/v1alpha1 kind: Astarte metadata: name: astarte namespace: astarte spec: # This is the most minimal set of reasonable configuration to spin up an Astarte # instance with reasonable defaults and enough control over the deployment. version: 1.1.1 api: host: &quot;api.astarte.yourdomain.com&quot; # MANDATORY rabbitmq: resources: requests: cpu: 300m memory: 512M limits: cpu: 1 memory: 1000M # this configuration deploys cassandra in cluster. This is not advised for production environments cassandra: maxHeapSize: 1024M heapNewSize: 256M storage: size: 30Gi resources: requests: cpu: 1 memory: 1024M limits: cpu: 2 memory: 2048M vernemq: host: &quot;broker.astarte.yourdomain.com&quot; sslListener: true sslListenerCertSecretName: &lt;your-tls-secret&gt; resources: requests: cpu: 200m memory: 1024M limits: cpu: 1000m memory: 2048M cfssl: resources: requests: cpu: 100m memory: 128M limits: cpu: 200m memory: 256M storage: size: 2Gi components: # Global resource allocation. Automatically allocates resources to components weighted in a # reasonable way. resources: requests: cpu: 1200m memory: 3072M limits: cpu: 3000m memory: 6144M Starting from Astarte v1.0.1, traffic coming to the broker is TLS terminated ad VerneMQ level. The two fields controlling this features, namely sslListener and sslListenerCertSecretName can be found within the vernemq section of the Astarte CR. In a nutshell, their meaning is: sslListener controls whether TLS termination is enabled at VerneMQ level or not, sslListenerCertSecretName is the name of TLS secret used for TLS termination (more on how to deal with Astarte certificates here ). When sslListener is true, the secret name must be set. You can simply apply this resource in your Kubernetes cluster with kubectl apply -f &lt;astarte-cr.yaml&gt; . The Operator will take over from there.","ref":"060-setup_cluster.html#using-a-standard-astarte-cr","title":"Setting up the Cluster - Using a standard Astarte CR","type":"extras"},{"doc":"Once your Cluster is up and running , to expose it to the outer world you need to set up an Ingress. Currently, the only managed and supported Ingress is based upon NGINX , and this guide will cover only this specific case. Please, note that the support for the Voyager based ingress (i.e.: AstarteVoyagerIngress) has been removed as of v24.5.0. If you want to migrate away from AstarteVoyagerIngress to the new AstarteDefaultIngress, please refer to the procedure outlined here .","ref":"064-setup_astartedefaultingress.html","title":"Setting up the Astarte Default Ingress","type":"extras"},{"doc":"Before proceeding with the deployment of the Astarte Default Ingress, the following requirements must be fulfilled: TLS certificates must be deployed as a secret within the namespace in which Astarte resides (see the Handling Astarte Certificates section). To check if the TLS secret is correctly deployed, you can run: $ kubectl get secrets -n astarte and make sure your certificate is stored in a secret of type kubernetes.io/tls in that list; Astarte must be configured such that TLS termination is handled at VerneMQ level: this can be done simply editing the Astarte resource and, in the vernemq section, setting the sslListener and sslListenerCertSecretName . Your Astarte CR will look something like: apiVersion: api.astarte-platform.org/v1alpha2 kind: Astarte ... spec: ... vernemq: sslListener: true sslListenerCertSecretName: &lt;your-tls-secret-name&gt; ... ingress-nginx ingress controller must be deployed within your cluster. You can install it following the instructions reported here .","ref":"064-setup_astartedefaultingress.html#prerequisites","title":"Setting up the Astarte Default Ingress - Prerequisites","type":"extras"},{"doc":"Most information needed for exposing your Ingress have already been given in your main Astarte resource. If your Kubernetes installation supports LoadBalancer ingresses (most managed ones do), you should be able to get away with the most standard CR: apiVersion: ingress.astarte-platform.org/v1alpha1 kind: AstarteDefaultIngress metadata: name: adi namespace: astarte spec: ### Astarte Default Ingress CRD astarte: astarte tlsSecret: &lt;your-astarte-tls-cert&gt; api: exposeHousekeeping: true dashboard: deploy: true ssl: true host: &lt;your-astarte-dashboard-host&gt; broker: deploy: true serviceType: LoadBalancer # loadBalancerIP is needed if your certificate is obtained with the solution of the HTTP # challenge, otherwise it's optional. Please, be aware that the possibilities and modes for # assigning a loadBalancerIP to a service depend on your cloud provider. loadBalancerIP: &lt;your-loadbalancerIP&gt; There's one very important thing to be noted: the astarte field must reference the name of an existing Astarte installation in the same namespace, and the Ingress will be configured and attached to that instance.","ref":"064-setup_astartedefaultingress.html#creating-an-astartedefaultingress","title":"Setting up the Astarte Default Ingress - Creating an AstarteDefaultIngress","type":"extras"},{"doc":"When the AstarteDefaultIngress resource is created, the Astarte Operator ensures that the following resources are created according to your configuration: an NGINX ingress which is devoted to routing requests to the Astarte APIs and to the Astarte Dashboard, a service of kind LoadBalancer which exposes the Astarte broker to the outer world. The following commands will help you in the task of retrieving the external IPs assigned to both the ingress and the broker service. Assuming that your Astarte instance and AstarteDefaultIngress are respectively named astarte and adi , and that they are deployed within the astarte namespace, simply run: $ # retrieve information about the ingress $ kubectl get ingress -n astarte NAME CLASS HOSTS ADDRESS PORTS AGE adi-api-ingress nginx &lt;your-hosts&gt; X.Y.W.Z 80, 443 6s and $ # retrieve information about the broker service $ kubectl get service -n astarte adi-broker-service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE adi-broker-service LoadBalancer x.x.x.x A.B.C.D 443:32149/TCP 17s","ref":"064-setup_astartedefaultingress.html#what-happens-after-installing-the-astartedefaultingress-resource","title":"Setting up the Astarte Default Ingress - What happens after installing the AstarteDefaultIngress resource?","type":"extras"},{"doc":"Astarte heavily requires SSL in a number of interactions, even though this can be bypassed with ssl: false . If you do not have any SSL certificates for your domains, you can leverage cert-manager capabilities. Simply follow the instructions outlined here to learn how to handle your certificates.","ref":"064-setup_astartedefaultingress.html#ssl-and-certificates","title":"Setting up the Astarte Default Ingress - SSL and Certificates","type":"extras"},{"doc":"When your certificate is issued after the solution of an HTTP challenge, to ensure the renewal of the certificate itself you must ensure that the NGINX ingress and the broker service are exposed on the same external IP. Given that the ingress external IP is obtained after the deployment of the NGINX ingress controller, all you have to do is ensuring that the broker service is exposed on the ingress IP. Thus, set the loadBalancerIP field in your AstarteDefaultIngress resource: apiVersion: ingress.astarte-platform.org/v1alpha1 kind: AstarteDefaultIngress ... spec: ... broker: deploy: true serviceType: LoadBalancer loadBalancerIP: &lt;same-IP-of-your-ingress&gt; Please, be aware that the possibility of setting the loadBalancerIP is dependent on your cloud provider. For example, if your Astarte instance is hosted by Google, you will need to reserve the IP before assigning it to the broker service (see this page for further details). Discussing how other cloud providers handle this specific task is out of the scope of this guide and is left to the reader.","ref":"064-setup_astartedefaultingress.html#how-to-support-automatic-certificate-renewal-for-http-challenges","title":"Setting up the Astarte Default Ingress - How to support automatic certificate renewal for HTTP challenges?","type":"extras"},{"doc":"AstarteDefaultIngress deploys a well-known tree of APIs to the host you specified in the main Astarte resource. In particular, assuming your API host was api.astarte.yourdomain.com : Housekeeping API base URL will be: https://api.astarte.yourdomain.com/housekeeping Realm Management API base URL will be: https://api.astarte.yourdomain.com/realmmanagement Pairing API base URL will be: https://api.astarte.yourdomain.com/pairing AppEngine API base URL will be: https://api.astarte.yourdomain.com/appengine","ref":"064-setup_astartedefaultingress.html#api-paths","title":"Setting up the Astarte Default Ingress - API Paths","type":"extras"},{"doc":"AstarteDefaultIngress has a number of advanced options that can be used to accommodate needs of the most diverse deployments. Consult the CRD Documentation to learn more.","ref":"064-setup_astartedefaultingress.html#further-customization","title":"Setting up the Astarte Default Ingress - Further customization","type":"extras"},{"doc":"The support for AstarteVoyagerIngress has been dropped. If your Astarte deployment is still being served by the AstarteVoyagerIngress, it is necessary to move to the new managed ingress, namely, the AstarteDefaultIngress. As an alternative, you can setup your custom ingress (however, this scenario is not covered by the present documentation). The current page focuses on describing the procedure for migrating your ingress with ease, simply employing astartectl . Please, make sure you have read the entirety of this page and to understand all the concepts and the implications before performing the actual migration.","ref":"066-migrate_to_astartedefaultingress.html","title":"Migrating to the Astarte Default Ingress","type":"extras"},{"doc":"Before starting with the actual migration procedure, some preliminary activities are required: ensure that the version of the Astarte operator in your cluster is at least &gt;= v1.0.1 and stable. If this requirement is not fulfilled, please refer to the Upgrade Procedures section; the ingress-nginx ingress controller must be deployed in your cluster (see this section for the details). Be sure of taking note of the ingress-class of the controller which is meant to handle your Astarte ingress. This information will come handy during the migration itself; make sure the TLS secrets used to secure the communications to and from Astarte are deployed. For the details, please refer to the Handling Astarte certificates page; ensure that astartectl is installed on your machine and its version is at least &gt;= v1.0.0 .","ref":"066-migrate_to_astartedefaultingress.html#prerequisites-and-preliminary-checks","title":"Migrating to the Astarte Default Ingress - Prerequisites and preliminary checks","type":"extras"},{"doc":"Performing the actual ingress migration is as simple as executing an astartectl command: $ astartectl cluster instances migrate replace-voyager The replace-voyager command provides meaningful defaults so that, if your Astarte deployment relies on standard naming practices, you can simply omit all the flags. The following list of options is available: --namespace : the namespace in which the Astarte instance resides (default: &quot;astarte&quot;); --operator-name : the name of the Astarte Operator instance (default: &quot;astarte-operator-controller-manager&quot;); --operator-namespace : the namespace in which the Astarte Operator resides (default: &quot;kube-system&quot;); --ingress-name : the name of the AstarteVoyagerIngress to be migrated. When not set, the first ingress found in the cluster will be selected. To find the AstarteVoyagerIngress resources present in your cluster simply run: $ kubectl get avi -n &lt;astarte-namespace&gt; --out : the name of the file in which the AstarteVoyagerIngress custom resource will be saved (optional). To successfully complete the procedure, you will be prompted to interactively insert details such as: the name of the Astarte TLS secrets, the name of the to-be-installed AstarteDefaultIngress resource, the ingress class of the NGINX ingress controller. Before starting the migration routine, you will be asked to review the generated AstarteDefaultIngress custom resource. The migration will start only upon confirmation. Please, note that you can contextually dump your AstarteVoyagerIngress custom resource for backup purposes simply using the --out &lt;filename&gt; option. What happens under the hood? When invoking the replace-voyager command, astartectl interacts with your Astarte cluster and retrieves the AstarteVoyagerIngress resource which serves your Astarte instance. If no AstarteVoyagerIngresses are present, the procedure is immediately terminated as there is nothing to migrate. After all the required information is provided through the interactive prompt, the AstarteDefaultIngress resource is reviewed and the final confirmation is provided, the following tasks are performed: the Astarte resource is patched so that TLS termination is handled at the VerneMQ level: in particular, the fields sslListener and sslListenerCertSecretName are populated; the AstarteDefaultIngress resource is installed within your cluster. Once installed, the Astarte Operator takes over and ensures that: a service of kind LoadBalancer is created to serve the Astarte broker, an ingress resource is created to serve the Astarte APIs (and the Dashboard, if requested). If one of the previous tasks are not successful, the migration logic is reverted as not to leave your cluster in a broken state. At the end of the procedure, after the AstarteDefaultIngress is successfully created, the old AstarteVoyagerIngress resource will be deleted. Anyway, if any errors occur during the deletion of the AstarteVoyagerIngress, the migration procedure is not reverted (as the AstarteDefaultIngress resource is successfully deployed) and, as such, you are required to explicitly delete the AstarteVoyagerIngress resource by hand. Be aware that Voyager specific annotations cannot be mapped to the AstarteDefaultIngress. If any of those annotations are present, the replace-voyager command will print a warning message. It will be your responsibility confirming whether you want to proceed or abort the procedure.","ref":"066-migrate_to_astartedefaultingress.html#performing-the-migration","title":"Migrating to the Astarte Default Ingress - Performing the migration","type":"extras"},{"doc":"The current section focuses on some advanced configuration scenarios that might help you in handling specific non-standard use cases. Preserve the API and Broker IPs The need of preserving both the API and Broker IPs may arise in specific use cases when, for example, there are any number of impediments in updating your DNS zones. Please, be aware that the following instructions rely on the assumption that you can reserve (or you already reserved) external IPs. This task is highly dependent on your cloud service provider and, as such, you are required to ensure that it can be performed in your specific case. If your Astarte instance is exposed to the outer world through the AstarteVoyagerIngress, two external IPs are assigned to your services: one for the Broker and another for the Astarte APIs (and the dashboard, if deployed). Before migrating your ingress to the AstarteDefaultIngress, perform the following operations: ensure that both the API and the Broker IPs are reserved, patch your AstarteVoyagerIngress resource such that the loadBalancerIP field is set for the broker (if this field is already set, you can skip this step): broker: loadBalancerIP: &lt;the-broker-reserved-IP&gt; ... install (or update, if already installed) the ingress-nginx deployment by explicitly setting the IP for the ingress controller as to make it coincident with the IP assigned to the Astarte APIs. Be aware that, at first, the ingress controller external IP will remain in pending state until the AstarteVoyagerIngress will be deleted and, once that IP will be available, it will be correctly assigned to the ingress controller. $ helm upgrade --install &lt;ingress-nginx-name&gt; ingress-nginx/ingress-nginx \\ -n ingress-nginx \\ --set controller.service.externalTrafficPolicy=Local \\ --set controller.service.loadBalancerIP=&lt;the-API-reserved-IP&gt; Once the previous instructions are executed, you are ready to perform the migration to the AstarteDefaultIngress as described in the Performing the Migration section. As a final remark, if you are interested in preserving only one of the external IPs, please refer only to the instructions that apply to your needs (e.g.: the broker) while neglecting the remaining parts.","ref":"066-migrate_to_astartedefaultingress.html#advanced-use-cases","title":"Migrating to the Astarte Default Ingress - Advanced use cases","type":"extras"},{"doc":"This section provides guidance on some delicate operations that must be performed manually as they could potentially result in data loss or other types of irrecoverable damage. Always be careful while performing these operations! Advanced operations are described in the following sections: How to backup your Astarte resources How to restore your backed up Astarte instance","ref":"095-advanced_operations.html","title":"Advanced operations","type":"extras"},{"doc":"Backing up your Astarte resources is crucial in all those cases when your Astarte instance has to be restored after an unforeseen event (e.g. accidental deletion of resources, deletion of the Operator - as it will be discussed later on - etc.). A full recovery of your Astarte instance along with all the persisted data is possible if and only if your Cassandra/Scylla instance is deployed independently from Astarte, i.e. it must be deployed outside of the Astarte CR scope. Provided that this condition is met, all the data persist in the database even when Astarte is deleted from your cluster. To restore your Astarte instance all you have to do is saving the following resources: Astarte CR; AstarteDefaultIngress CR (if deployed); CA certificate and key; and, assuming that the name of your Astarte is astarte and that it is deployed within the astarte namespace, it can be done simply executing the following commands: kubectl get astarte -n astarte -o yaml &gt; astarte-backup.yaml kubectl get adi -n astarte -o yaml &gt; adi-backup.yaml kubectl get secret astarte-devices-ca -n astarte -o yaml &gt; astarte-devices-ca-backup.yaml","ref":"095-advanced_operations.html#backup-your-astarte-resources","title":"Advanced operations - Backup your Astarte resources","type":"extras"},{"doc":"To restore your Astarte instance simply apply the resources you saved as described here . Please, be aware that the order of the operations matters. kubectl apply -f astarte-devices-ca-backup.yaml kubectl apply -f astarte-backup.yaml And when your Astarte resource is ready, to restore your AstarteDefaultIngress resource: kubectl apply -f adi-backup.yaml At the end of this step, your cluster is restored. Please, notice that the external IP of the ingress services might have changed. Take action to ensure that the changes of the IP are reflected anywhere appropriate in your deployment.","ref":"095-advanced_operations.html#restore-your-backed-up-astarte-instance","title":"Advanced operations - Restore your backed up Astarte instance","type":"extras"},{"doc":"The procedure for upgrading Astarte and Astarte Operator depends on the version you want to upgrade from. After Astarte Operator v1.0.x, a change in the versioning scheme was introduced. This does not have any impact on the Astarte version. Refer to the related issue for more information. Find below the upgrade guides for your Astarte cluster: to upgrade from v0.10 to v0.11, click here to upgrade from v0.11 to v1.0, click here to upgrade from v1.0.0 to v1.0.x, click here to upgrade from v1.0.x to v22.11, click here to upgrade from v22.11.x to v23.5, click here","ref":"000-upgrade_index.html","title":"Upgrade Procedures","type":"extras"},{"doc":"The upgrade procedure for both Astarte and Astarte Operator v0.10 is handled by astartectl , which is the tool to be employed to upgrade from v0.10 to v0.11 .","ref":"010-upgrade_010_011.html","title":"Upgrade v0.10-v0.11","type":"extras"},{"doc":"To upgrade the Operator, use the dedicated upgrade-operator command: astartectl cluster upgrade-operator This command upgrades the Operator to the last v0.11.x version available. However, for specific and non-standard use cases, the --version switch is provided to allow the user to specify the version to upgrade to. We highly encourage you to follow the standard approach and to land to the last v0.11.x version available.","ref":"010-upgrade_010_011.html#upgrade-astarte-operator","title":"Upgrade v0.10-v0.11 - Upgrade Astarte Operator","type":"extras"},{"doc":"To upgrade Astarte use the dedicated command: astartectl cluster instances upgrade &lt;your-astarte-release-name&gt; The command upgrades your Astarte instance to the last 0.11.x version available. The Astarte version to upgrade to can be set by the user. Please, see astartectl cluster instances upgrade --help for further details. Unless you have specific reasons, it is highly recommended upgrading Astarte to the last 0.11 version available.","ref":"010-upgrade_010_011.html#upgrade-astarte","title":"Upgrade v0.10-v0.11 - Upgrade Astarte","type":"extras"},{"doc":"","ref":"020-upgrade_011_10.html","title":"Upgrade v0.11-v1.0","type":"extras"},{"doc":"The upgrade procedure from v0.11 to v1.0 requires some manual intervention as the deployment and handling of the Operator's lifecycle has changed: if v0.11 is entirely handled with astartectl , v1.0 employs Helm charts. Helm is intended to be used as the operator's lifecycle management tool, thus make sure you are ready with a working Helm installation . Migrate CA certificate and key Currently you have to manually migrate the CA certificate and private key to the new installation. This is critical since the devices in your realm have certificates signed with those CA credentials, and if the CA certificate changes all devices will have invalid credentials and will have to request new credentials. tar must be installed onto your machine in order to successfully complete this step. Export the following enviroment variables and ensure they matches the name of the corresponding components within your cluster: ASTARTE_RELEASE_NAME : the name of the Astarte instance deployed in your cluster, ASTARTE_RELEASE_NAMESPACE : the namespace in which your Astarte instance resides. For instance, if you followed the standard naming conventions while installing Astarte it should be sufficient setting the following values: export ASTARTE_RELEASE_NAME=astarte export ASTARTE_RELEASE_NAMESPACE=astarte However, it is your responsibility checking that these values match the ones of your cluster . Now, migrate the CA certificates and key: kubectl cp $ASTARTE_RELEASE_NAMESPACE/$ASTARTE_RELEASE_NAME-cfssl-0:/data/ca-key.pem tls.key kubectl cp $ASTARTE_RELEASE_NAMESPACE/$ASTARTE_RELEASE_NAME-cfssl-0:/data/ca.pem tls.crt kubectl create secret tls $ASTARTE_RELEASE_NAME-devices-ca -n $ASTARTE_RELEASE_NAMESPACE \\ --cert=tls.crt --key=tls.key Remove Astarte Operator v0.11 Remove the Operator's Service Account, Cluster Roles and Cluster Role Bindings: kubectl delete serviceaccounts -n kube-system astarte-operator kubectl delete clusterroles.rbac.authorization.k8s.io astarte-operator kubectl delete clusterrolebindings.rbac.authorization.k8s.io astarte-operator Delete the Operator's deployment: kubectl delete deployments.app -n kube-system astarte-operator DO NOT delete Astarte's CRDs! This will lead to the deletion of the entire Astarte deployment with a consequent data loss. Install cert-manager Please, before proceeding to the next steps make sure to be compliant with the new requirements for v1.0 , i.e. if cert-manager is not installed yet, run the following commands: helm repo add jetstack https://charts.jetstack.io helm repo update kubectl create namespace cert-manager helm install \\ cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --version v1.7.0 \\ --set installCRDs=true Add Astarte Operator's Helm Chart Repository To restore the Operator's functionalities with v1.0, the first step is adding the Astarte Operator's Helm chart repository: helm repo add astarte https://helm.astarte-platform.org helm repo update Prepare the Cluster to the Operator v1.0 Installation Set some environment variables that will come handy for the upcoming migration procedure: ASTARTE_OP_TEMPLATE_DIR is the target directory in which the chart templates will be generated, ASTARTE_OP_RELEASE_NAME is the name of the Astarte Operator deployment, ASTARTE_OP_RELEASE_NAMESPACE is the namespace in which the Astarte Operator will reside. You can change the following values at your preference. However, the following values should be ok for the large majority of use cases. Note that you are responsible for checking that ASTARTE_OP_RELEASE_NAMESPACE exists within the cluster. export ASTARTE_OP_TEMPLATE_DIR=/tmp export ASTARTE_OP_RELEASE_NAME=astarte-operator export ASTARTE_OP_RELEASE_NAMESPACE=kube-system Generate the Helm templates with the following: helm template $ASTARTE_OP_RELEASE_NAME astarte/astarte-operator \\ --namespace $ASTARTE_OP_RELEASE_NAMESPACE \\ --output-dir $ASTARTE_OP_TEMPLATE_DIR The outcome of this command consists of a series of yaml files located in $ASTARTE_OP_TEMPLATE_DIR/astarte-operator/templates . Leveraging the templating capabilities of Helm, the generated templates can be customized according to your needs. For instance, if you want to refer to a particular Operator's tag all you have to do is to append --set image.tag=&lt;the-required-tag&gt; to the previous command. To check all the configurable values run helm show values astarte/astarte-operator . Before moving on, make sure that gawk is installed on your host machine. If you are on OSX, running the following command will be sufficient: $ brew install gawk while on any Debian based OS run: # apt install gawk Now it's time to prepare the cluster to allow the new operator installation by means of the templates generated in the previous step. The following tasks will be performed: replacement of the Astarte's and AstarteVoyagerIngress' CRDs with their updated versions, installation of the Flow's CRD, installation of the Operator's RBACs, installation of the Operator's Webhooks, annotation of the installed resources as to allow Helm to manipulate and take control of them. This bash script takes care of handling the aforementioned tasks. Download it and, assuming you saved it as upgrade-operator-011-10.sh , run the following: bash path/to/your/upgrade-operator-011-10.sh \\ -d $ASTARTE_OP_TEMPLATE_DIR \\ -n $ASTARTE_OP_RELEASE_NAME \\ -N $ASTARTE_OP_RELEASE_NAMESPACE In a more concise way you can perform the same task simply executing: ASTARTE_OP_UPGRADE_SCRIPT_URL=https://raw.githubusercontent.com/astarte-platform/astarte-kubernetes-operator/master/hack/upgrade-operator-011-10.sh curl -fsSL $ASTARTE_OP_UPGRADE_SCRIPT_URL &gt; /tmp/upgrade-operator-011-10.sh bash /tmp/upgrade-operator-011-10.sh \\ -d $ASTARTE_OP_TEMPLATE_DIR \\ -n $ASTARTE_OP_RELEASE_NAME \\ -N $ASTARTE_OP_RELEASE_NAMESPACE Install Astarte Operator v1.0 Now it's time to install the Astarte Operator v1.0. If during the preparation of the cluster you customized the chart values with the --set flag, please take care of setting the same values accordingly while installing Astarte Operator. To install the Operator, simply run: helm install $ASTARTE_OP_RELEASE_NAME astarte/astarte-operator -n $ASTARTE_OP_RELEASE_NAMESPACE \\ --skip-crds Note that the --skip-crds flag is required as, following the migration path, we already updated/installed the required CRDs. Version 1.0.0 is a safe landing version to perform the upgrade to. Upgrading to a more recent version may lead to a broken state with possible catastrophic aftermaths: if you choose to follow this path, make sure you know what you are doing. However deviating from the upgrade path outlined within the current page is strongly discouraged. After the successful migration to v1.0, please upgrade to a more recent Operator's versions following the instructions outlined in the Upgrade Operator v1.0.0-v1.0.x section. Caveats and Breaking Changes v0.11-v1.0 Operator v1.0 introduced some breaking changes with respect to v0.11, which are relevant if you deployed your own Ingress instead of relying on AstarteVoyagerIngress . With the upgrade to v1.0, some of the services were renamed: $ASTARTE_RELEASE_NAME-appengine --&gt; $ASTARTE_RELEASE_NAME-appengine-api $ASTARTE_RELEASE_NAME-housekeeping --&gt; $ASTARTE_RELEASE_NAME-housekeeping-api $ASTARTE_RELEASE_NAME-pairing --&gt; $ASTARTE_RELEASE_NAME-pairing-api $ASTARTE_RELEASE_NAME-realm-management --&gt; $ASTARTE_RELEASE_NAME-realm-management-api If you deployed your own Ingress, it is your responsibility renaming your services to ensure the cluster to be fully operational.","ref":"020-upgrade_011_10.html#upgrade-astarte-operator","title":"Upgrade v0.11-v1.0 - Upgrade Astarte Operator","type":"extras"},{"doc":"Once you migrated Astarte Operator from v0.11 to v1.0, it is time to upgrade your Astarte instance. To do so, simply edit the Astarte resource in the cluster updating the version field to the one you want to upgrate to. Open the yaml file describing the Astarte resource with: kubectl edit astarte -n astarte Find the version field in the Astarte Spec section and change it according to your needs. Once the yaml file will be saved, the Operator will take over ensuring the reconciliation of your Astarte instance to the requested version. Caveats CFSSL leftover persistent volume Astarte v0.11 employs a persistent volume to store CA certificate and private key, while upgrading to v1.0 involves a change in how device certificates are stored as, behind the scenes, these data are held as a Kubernetes TLS secret. Following the Kubernetes conventions, the formerly used persistent volume and its corresponding claim are left within the cluster even if not used anymore. If you followed the procedure described here you are free to remove the CFSSL persistent volume and claim without the need for your devices to request new credentials. AppEngine /socket route removal The /socket endpoint exposed by AppEngine to interact with Astarte Channels, which was already deprecated in Astarte v0.11, has been removed. You must use the new route /v1/socket instead.","ref":"020-upgrade_011_10.html#upgrade-astarte","title":"Upgrade v0.11-v1.0 - Upgrade Astarte","type":"extras"},{"doc":"The current section describes the required steps to upgrade your Astarte instance from v1.0.0 to v1.0.x . Currently, the last released patch version is v1.0.3 and, as such, the remainder of this page will refer to this version. The described upgrade path involves some heavy changes as a consequence of this Voyager announcement and the following Astarte design choice . Before moving on, it must be clear that AstarteVoyagerIngress is deprecated and that the only supported managed ingress is the AstarteDefaultIngress . The upcoming sections will cover the following topics: upgrading the Astarte Operator, upgrading the Astarte instance to allow for TLS termination at VerneMQ level, deployment of the AstarteDefaultIngress in place of the deprecated AstarteVoyagerIngress . Before starting with the upgrade procedure it is strongly advised to backup your Astarte resources .","ref":"030-upgrade_100_10x.html","title":"Upgrade v1.0.0-v1.0.x","type":"extras"},{"doc":"Astarte Operator's upgrade procedure is handled by Helm. However, according to the Helm policies, upgrading the CRDs must be handled manually. The current section assumes that the Operator's chart landing version is v1.0.3 . If a more recent chart version is available, it is your responsibility referencing to the v1.0.3 chart using the --version flag when running helm commands. To upgrade the Astarte CRDs, the following environment variables will be employed: ASTARTE_OP_TEMPLATE_DIR is the target directory in which the chart templates will be generated, ASTARTE_OP_RELEASE_NAME is the name of the Astarte Operator deployment, ASTARTE_OP_RELEASE_NAMESPACE is the namespace in which the Astarte Operator resides. Please, make sure that the values you set for both the Operator's name and namespace match the naming you already adopted when installing the Operator. A wrong naming can lead to a malfunctioning Astarte cluster. For standard deployments the following variables should be ok. However, it is your responsibility checking that the values you set are consistent with your setup: export ASTARTE_OP_TEMPLATE_DIR=/tmp export ASTARTE_OP_RELEASE_NAME=astarte-operator export ASTARTE_OP_RELEASE_NAMESPACE=kube-system Update your local Helm charts: $ helm repo update Render the Helm templates with the following: helm template $ASTARTE_OP_RELEASE_NAME astarte/astarte-operator \\ --namespace $ASTARTE_OP_RELEASE_NAMESPACE \\ --output-dir $ASTARTE_OP_TEMPLATE_DIR After these steps you will find the updated CRDs within $ASTARTE_OP_TEMPLATE_DIR/$ASTARTE_OP_RELEASE_NAME/templates/crds.yaml . Update the CRDs in your cluster by replacing the CRDs yaml file: kubectl replace -f $ASTARTE_OP_TEMPLATE_DIR/$ASTARTE_OP_RELEASE_NAME/templates/crds.yaml The previous command will raise an error saying customresourcedefinitions.apiextensions.k8s.io &quot;astartedefaultingresses.ingress.astarte-platform.org&quot; not found . It is nothing to worry about: under the hood the replace command has updated the CRDs for Astarte, AstarteVoyagerIngress and Flow, while it cannot replace the AstarteDefaultIngress CRD as it is not installed yet. This issue is easily fixed with the next command. To upgrade the Operator use the dedicated helm upgrade command: helm upgrade astarte-operator astarte/astarte-operator -n kube-system The optional --version switch allows to specify the version to upgrade to - when not specified, the latest version will be fetched and used. If you choose to upgrade to a specific version of the chart by using the --version flag, please make sure to generate the updated CRDs template using the same chart version. By design, Astarte Operator's Helm charts cannot univocally be mapped to Operator's releases in a one-to-one relationship. However each chart is tied to a specific Operator's version, which is user configurable. Therefore, upgrading a chart leads to an Operator's upgrade if and only if the Operator's tag referenced by the chart is changed. You can check the Operator's tag bound to the chart simply running: helm show values astarte/astarte-operator As usual, you can use the usual --version flag to point to a specific chart version.","ref":"030-upgrade_100_10x.html#upgrade-astarte-operator","title":"Upgrade v1.0.0-v1.0.x - Upgrade Astarte Operator","type":"extras"},{"doc":"To upgrade your Astarte instance simply edit the Astarte resource in the cluster updating the version field to the one you want to upgrade to. In order to properly expose your Astarte instance to the outer world through the AstarteDefaultIngress a configuration change for VerneMQ is required: in particular, TLS termination must be handled at VerneMQ level. Open the yaml file describing the Astarte resource with: kubectl edit astarte -n astarte Find the version field in the Astarte Spec section and change it according to your needs (i.e.: set it to 1.0.3 ). Moreover, in the vernemq configuration section two new fields must be added, namely sslListener and sslListenerCertSecretName : the first field is a boolean that, when true, set VerneMQ to handle TLS termination, while the latter set the secret containing the TLS certificate (further details on certificates can be found here ). To summarize, the needed changes will look like the following sample snippet: apiVersion: api.astarte-platform.org/v1alpha2 kind: Astarte ... spec: ... vernemq: sslListener: true sslListenerCertSecretName: &lt;your-tls-secret-name&gt; ... version: 1.0.3 Once the yaml file is applied, the Operator will take over ensuring the reconciliation of your Astarte instance. Caveats for Astarte Flow Currently, although Astarte Flow is a component of Astarte, it doesn't follow Astarte's release cycle. Therefore if you upgraded your Astarte instance to v1.0.3, Astarte Operator will try to deploy astarte/astarte_flow:1.0.3 which is currently not existent. All you have to do to overcome this temporary limitation is to edit your Astarte resource by explicitly setting the Astarte Flow image you plan to use: spec: ... components: ... flow: image: &lt;the-astarte-flow-image&gt; All the available Astarte Flow's tags can be found here .","ref":"030-upgrade_100_10x.html#upgrade-astarte","title":"Upgrade v1.0.0-v1.0.x - Upgrade Astarte","type":"extras"},{"doc":"The current section describes the procedure for replacing the deprecated AstarteVoyagerIngress with the new AstarteDefaultIngress . If the Voyager ingress is not deployed within your cluster, feel free to skip this section. The advised migration path involves the employment of astartectl : this is the most straightforward way of performing the migration task and, as soon as all the requirements are satisfied, it requires the execution of one single command. The Migrating to the AstarteDefaultIngress page extensively cover this topic.","ref":"030-upgrade_100_10x.html#deploy-astartedefaultingress-in-place-of-astartevoyageringress","title":"Upgrade v1.0.0-v1.0.x - Deploy AstarteDefaultIngress in place of AstarteVoyagerIngress","type":"extras"},{"doc":"This page describes the required steps to upgrade your Astarte cluster from v1.0.x to v22.11.x . Your Astarte instance will not need to be upgraded. The change in the versioning scheme of the Astarte Operator is meant to better keep up with Kubernetes release cycle (see the related issue for more information). Starting from the Astarte Operator v22.11 release, the old api.astarte-platform.org/v1alpha1 APIs are deprecated and will be removed in the next release. In the following, the upgrade path is described. The upcoming sections will cover the following topics: upgrading the Astarte Operator, making sure that the Astarte, AstarteVoyagerIngress and Flow CR are stored using v1alpha2 API version, upgrading the Astarte, AstarteVoyagerIngress and Flow CRDs to have only v1alpha2 as storage version. Before starting with the upgrade procedure it is strongly advised to backup your Astarte resources .","ref":"040-upgrade_10x_2211.html","title":"Upgrade v1.0.x-v22.11.x","type":"extras"},{"doc":"The Astarte Operator upgrade procedure is handled by Helm. The current section assumes that the Operator's chart landing version is v22.11.x . It is your responsibility referencing the proper v22.11.x chart using the --version flag when running helm commands. Please, make sure that the values you set for both the Operator's name and namespace match the naming you already adopted when installing the Operator. A wrong naming can lead to a malfunctioning Astarte cluster. For standard deployments the following variables should be ok. However, it is your responsibility checking that the values you set are consistent with your setup: export ASTARTE_OP_RELEASE_NAME=astarte-operator export ASTARTE_OP_RELEASE_NAMESPACE=astarte-operator export ASTARTE_OP_CHART_VERSION=&lt;22.11.x&gt; Update your local Helm charts: helm repo update To upgrade the Operator use the dedicated helm upgrade command: helm upgrade $ASTARTE_OP_RELEASE_NAME astarte/astarte-operator -n $ASTARTE_OP_RELEASE_NAMESPACE \\ --version $ASTARTE_OP_CHART_VERSION The optional --version switch allows to specify the version to upgrade to - when not specified, the latest version will be fetched and used. By design, Astarte Operator's Helm charts cannot univocally be mapped to Operator's releases in a one-to-one relationship. However each chart is tied to a specific Operator's version, which is user configurable. Therefore, upgrading a chart leads to an Operator's upgrade if and only if the Operator's tag referenced by the chart is changed. You can check the Operator's tag bound to the chart simply running: helm show values astarte/astarte-operator As usual, you can use the --version flag to point to a specific chart version.","ref":"040-upgrade_10x_2211.html#upgrade-astarte-operator","title":"Upgrade v1.0.x-v22.11.x - Upgrade Astarte Operator","type":"extras"},{"doc":"To do so, simply edit the Astarte resource in the cluster. Open the yaml file describing the Astarte resource with: kubectl edit astarte -n astarte Find the apiVersion field in the Astarte Spec section and change it (if needed) to api.astarte-platform.org/v1alpha2 . After having done this, you Astarte CR will look like this: apiVersion: api.astarte-platform.org/v1alpha2 kind: Astarte ... spec: ... status: ... Once the yaml file is applied, the Operator will take over ensuring the reconciliation of your Astarte instance. This will in turn change the version in which the Astarte CR is stored in Kubernetes to api.astarte-platform.org/v1alpha2 .","ref":"040-upgrade_10x_2211.html#make-sure-astarte-is-stored-using-the-api-astarte-platform-org-v1alpha2-apiversion","title":"Upgrade v1.0.x-v22.11.x - Make sure Astarte is stored using the api.astarte-platform.org/v1alpha2 apiVersion","type":"extras"},{"doc":"To do so, simply edit the Flow resource in the cluster. Open the yaml file describing the Flow resource with: kubectl edit flow -n astarte Find the apiVersion field in the Flow Spec section and change it (if needed) to api.astarte-platform.org/v1alpha2 . After having done this, your Flow CR will look like this: apiVersion: api.astarte-platform.org/v1alpha2 kind: Flow ... spec: ... status: ... Once the yaml file is applied, the Operator will take over ensuring the reconciliation of your Flow instance. This will in turn change the version in which the Flow CR is stored in Kubernetes to api.astarte-platform.org/v1alpha2 .","ref":"040-upgrade_10x_2211.html#make-sure-flow-is-stored-using-the-api-astarte-platform-org-v1alpha2-apiversion","title":"Upgrade v1.0.x-v22.11.x - Make sure Flow is stored using the api.astarte-platform.org/v1alpha2 apiVersion","type":"extras"},{"doc":"The AstarteVoyagerIngress is deprecated and will be removed starting from Astarte Operator v23.5.0. Please consider switching to the new AstarteDefaultIngress . If you already have dropped the AstarteVoyagerIngress, you can skip this section. To do so, simply edit the AstarteVoyagerIngress resource in the cluster. Open the yaml file describing the AstarteVoyagerIngress resource with: kubectl edit avi -n astarte Find the apiVersion field in the AstarteVoyagerIngress Spec section and change it (if needed) to api.astarte-platform.org/v1alpha2 . After having done this, your AstarteVoyagerIngress CR will look like this: apiVersion: api.astarte-platform.org/v1alpha2 kind: AstarteVoyagerIngress ... spec: ... status: ... Once the yaml file is applied, the Operator will take over ensuring the reconciliation of your AstarteVoyagerIngress instance. This will in turn change the version in which the AstarteVoyagerIngress CR is stored in Kubernetes to api.astarte-platform.org/v1alpha2 .","ref":"040-upgrade_10x_2211.html#make-sure-the-apiversion-of-astartevoyageringress-is-api-astarte-platform-org-v1alpha2","title":"Upgrade v1.0.x-v22.11.x - Make sure the apiVersion of AstarteVoyagerIngress is api.astarte-platform.org/v1alpha2","type":"extras"},{"doc":"This step is handled by astartectl . Ensure that astartectl is installed on your machine and its version is at least &gt;= v22.11.00 . Performing the actual upgrade is as simple as executing the following command: $ astartectl cluster instances migrate storage-version If an error occurs during the migration procedure, changes made by migrate storage-version will be reverted, so that you can make sure of having performed all necessary steps described in this page before trying again. What happens under the hood? When invoking the migrate storage-version command, astartectl interacts with your Astarte cluster and retrieves the Astarte, Flow and AVI CRDs which are installed. Then, it checks that the CRDs are in a state consistent with the migration step it needs to perform, i.e. that each one of them has both v1alpha1 and v1alpha2 as storedVersions . For example, your Astarte CRD will look like this: name: astartes.api.astarte-platform.org ... spec: ... status: ... storedVersions: - v1alpha1 - v1alpha2 Then, astartectl simply shrinks the status.storedVersions field to just v1alpha2 . After having run the command, your Astarte CRD will look like this: name: astartes.api.astarte-platform.org ... spec: ... status: ... storedVersions: - v1alpha2 If one of the previous tasks is not successful, the migration logic is reverted as not to leave your cluster in a broken state.","ref":"040-upgrade_10x_2211.html#upgrade-the-crds-to-the-new-storage-version","title":"Upgrade v1.0.x-v22.11.x - Upgrade the CRDs to the new storage version","type":"extras"},{"doc":"This page describes the required steps to upgrade your Astarte cluster from v22.11.x to v23.5.x . Your Astarte instance will not need to be upgraded. Starting from the Astarte Operator v22.11 release, the old api.astarte-platform.org/v1alpha1 APIs are deprecated and will be removed in the next release ( v24.5 ). In the following, the upgrade path is described. The upcoming sections will cover the following topics: upgrading the Astarte Operator, Before starting with the upgrade procedure it is strongly advised to backup your Astarte resources .","ref":"050-upgrade_2211_235.html","title":"Upgrade v22.11.x-23.5x","type":"extras"},{"doc":"The Astarte Operator upgrade procedure is handled by Helm. The current section assumes that the Operator's chart landing version is v23.5.x . It is your responsibility referencing the proper v23.5.x chart using the --version flag when running helm commands. Please, make sure that the values you set for both the Operator's name and namespace match the naming you already adopted when installing the Operator. A wrong naming can lead to a malfunctioning Astarte cluster. For standard deployments the following variables should be ok. However, it is your responsibility checking that the values you set are consistent with your setup: export ASTARTE_OP_RELEASE_NAME=astarte-operator export ASTARTE_OP_RELEASE_NAMESPACE=astarte-operator export ASTARTE_OP_CHART_VERSION=&lt;23.5.x&gt; Update your local Helm charts: helm repo update To upgrade the Operator use the dedicated helm upgrade command: helm upgrade $ASTARTE_OP_RELEASE_NAME astarte/astarte-operator -n $ASTARTE_OP_RELEASE_NAMESPACE \\ --version $ASTARTE_OP_CHART_VERSION The optional --version switch allows to specify the version to upgrade to - when not specified, the latest version will be fetched and used. By design, Astarte Operator's Helm charts cannot univocally be mapped to Operator's releases in a one-to-one relationship. However each chart is tied to a specific Operator's version, which is user configurable. Therefore, upgrading a chart leads to an Operator's upgrade if and only if the Operator's tag referenced by the chart is changed. You can check the Operator's tag bound to the chart simply running: helm show values astarte/astarte-operator As usual, you can use the --version flag to point to a specific chart version.","ref":"050-upgrade_2211_235.html#upgrade-astarte-operator","title":"Upgrade v22.11.x-23.5x - Upgrade Astarte Operator","type":"extras"},{"doc":"This page describes the required steps to upgrade your Astarte cluster from v23.5.x to v24.5.x . Your Astarte instance will not need to be upgraded. [!WARNING] The old api.astarte-platform.org/v1alpha1 APIs have been removed ( v24.5 ). Make sure to follow the upgrade guide if you are still using the v1alpha1 APIs. In the following, the upgrade path is described. The upcoming sections will cover the following topics: upgrading the Astarte Operator, Before starting with the upgrade procedure it is strongly advised to backup your Astarte resources .","ref":"060-upgrade_235_245.html","title":"Upgrade v23.5.x-v24.5.x","type":"extras"},{"doc":"The Astarte Operator upgrade procedure is handled by Helm. The current section assumes that the Operator's chart landing version is v24.5.x . It is your responsibility referencing the proper v24.5.x chart using the --version flag when running helm commands. Please, make sure that the values you set for both the Operator's name and namespace match the naming you already adopted when installing the Operator. A wrong naming can lead to a malfunctioning Astarte cluster. For standard deployments the following variables should be ok. However, it is your responsibility checking that the values you set are consistent with your setup: export ASTARTE_OP_RELEASE_NAME=astarte-operator export ASTARTE_OP_RELEASE_NAMESPACE=astarte-operator export ASTARTE_OP_CHART_VERSION=&lt;24.5.x&gt; Update your local Helm charts: helm repo update To upgrade the Operator use the dedicated helm upgrade command: helm upgrade $ASTARTE_OP_RELEASE_NAME astarte/astarte-operator -n $ASTARTE_OP_RELEASE_NAMESPACE \\ --version $ASTARTE_OP_CHART_VERSION The optional --version switch allows to specify the version to upgrade to - when not specified, the latest version will be fetched and used. By design, Astarte Operator's Helm charts cannot univocally be mapped to Operator's releases in a one-to-one relationship. However each chart is tied to a specific Operator's version, which is user configurable. Therefore, upgrading a chart leads to an Operator's upgrade if and only if the Operator's tag referenced by the chart is changed. You can check the Operator's tag bound to the chart simply running: helm show values astarte/astarte-operator As usual, you can use the --version flag to point to a specific chart version.","ref":"060-upgrade_235_245.html#upgrade-astarte-operator","title":"Upgrade v23.5.x-v24.5.x - Upgrade Astarte Operator","type":"extras"},{"doc":"The Astarte Operator extends the Kubernetes API through the definition of Custom Resources. To browse the CRD documentation, follow this link .","ref":"001-intro_crds.html","title":"Introduction","type":"extras"}]